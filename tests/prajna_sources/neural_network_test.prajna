// ReLU activation
struct ReLU {
}

implement ReLU {
    @static
    func Forward(input: Tensor<f32, 2>)->Tensor<f32, 2> {
        var output = Tensor<f32, 2>::Create(input.Shape());
        var shape = input.Shape();
        
        for i in 0 to shape[0] {
            for j in 0 to shape[1] {
                var val = input[i, j];
                if (val > 0.0) {
                    output[i, j] = val;
                } else {
                    output[i, j] = 0.0;
                }
            }
        }
        return output;
    }
}

// Layer interface
interface Layer {
    func Forward(input: Tensor<f32, 2>)->Tensor<f32, 2>;
}

// Linear layer (fully connected layer)
struct LinearLayer {
    weights: Tensor<f32, 2>;
    biases: Tensor<f32, 2>;
    input_size: i64;
    output_size: i64;
}

implement LinearLayer {
    @static
    func Create(input_size: i64, output_size: i64)->LinearLayer {
        var layer: LinearLayer;
        layer.input_size = input_size;
        layer.output_size = output_size;
        
        // Initialize weights and biases
        layer.weights = Tensor<f32, 2>::Create([input_size, output_size]);
        layer.biases = Tensor<f32, 2>::Create([1, output_size]);
        
        // Xavier initialization for weights
        var scale = 1.0 / input_size.Cast<f32>();
        for i in 0 to input_size {
            for j in 0 to output_size {
                // Simple random initialization
                layer.weights[i, j] = (i * j % 100).Cast<f32>() * 0.01 * scale - 0.5 * scale;
            }
        }
        
        // Initialize biases to zero
        for j in 0 to output_size {
            layer.biases[0, j] = 0.0;
        }
        
        return layer;
    }
}

implement Layer for LinearLayer {
    func Forward(input: Tensor<f32, 2>)->Tensor<f32, 2> {
        var input_shape = input.Shape();
        var batch_size = input_shape[0];
        var output = Tensor<f32, 2>::Create([batch_size, this.output_size]);
        
        // Matrix multiplication: output = input * weights + bias
        for b in 0 to batch_size {
            for j in 0 to this.output_size {
                var sum = 0.0;
                for i in 0 to this.input_size {
                    sum = sum + input[b, i] * this.weights[i, j];
                }
                output[b, j] = sum + this.biases[0, j];
            }
        }
        
        return output;
    }
}

struct NeuralNetwork {
    layer1: LinearLayer;
    layer2: LinearLayer;
    layer3: LinearLayer;
    input_size: i64;
    output_size: i64;
}

implement NeuralNetwork {
    @static
    func Create(input_size: i64, output_size: i64)->NeuralNetwork {
        var network: NeuralNetwork;
        network.input_size = input_size;
        network.output_size = output_size;
        
        var linear1 = LinearLayer::Create(input_size, 4);
        var linear2 = LinearLayer::Create(4, 2);
        var linear3 = LinearLayer::Create(2, output_size);
        
        network.layer1 = linear1;
        network.layer2 = linear1;
        network.layer3 = linear1;
        
        return network;
    }
    
    func Forward(input: Tensor<f32, 2>)->Tensor<f32, 2> {
        // Forward pass through layer 1 + ReLU
        var h1 = this.layer1.Forward(input);
        var a1 = ReLU::Forward(h1);
        
        // Forward pass through layer 2 + ReLU
        var h2 = this.layer2.Forward(a1);
        var a2 = ReLU::Forward(h2);
        
        // Forward pass through layer 3 (output layer)
        var output = this.layer3.Forward(a2);
        
        return output;
    }
}

@test
func TestReLUActivation() {
    "Testing ReLU Activation...".PrintLine();
    
    var input = Tensor<f32, 2>::Create([2, 3]);
    input[0, 0] = -1.0; input[0, 1] = 0.0; input[0, 2] = 1.0;
    input[1, 0] = -2.0; input[1, 1] = 3.0; input[1, 2] = -0.5;
    
    var output = ReLU::Forward(input);
    
    "Input:".PrintLine();
    input.PrintLine();
    "ReLU Output:".PrintLine();
    output.PrintLine();
    
    test::Assert(output[0, 0] == 0.0);  // -1.0 -> 0.0
    test::Assert(output[0, 1] == 0.0);  // 0.0 -> 0.0
    test::Assert(output[0, 2] == 1.0);  // 1.0 -> 1.0
    test::Assert(output[1, 0] == 0.0);  // -2.0 -> 0.0
    test::Assert(output[1, 1] == 3.0);  // 3.0 -> 3.0
    test::Assert(output[1, 2] == 0.0);  // -0.5 -> 0.0
}

@test
func TestLinearLayer() {
    "Testing Linear Layer...".PrintLine();
    
    var layer = LinearLayer::Create(3, 2);
    var input = Tensor<f32, 2>::Create([2, 3]);
    
    // Set input values
    input[0, 0] = 1.0; input[0, 1] = 2.0; input[0, 2] = 3.0;
    input[1, 0] = 4.0; input[1, 1] = 5.0; input[1, 2] = 6.0;
    
    var output = layer.Forward(input);
    "Input:".PrintLine();
    input.PrintLine();
    "Output:".PrintLine();
    output.PrintLine();
    
    test::Assert(output.Shape()[0] == 2);
    test::Assert(output.Shape()[1] == 2);
}

@test
func TestNeuralNetwork() {
    "Testing Neural Network...".PrintLine();
    
    var network = NeuralNetwork::Create(2, 1);
    
    var input = Tensor<f32, 2>::Create([4, 2]);
    
    // XOR data
    input[0, 0] = 0.0; input[0, 1] = 0.0;
    input[1, 0] = 0.0; input[1, 1] = 1.0;
    input[2, 0] = 1.0; input[2, 1] = 0.0;
    input[3, 0] = 1.0; input[3, 1] = 1.0;
    
    // Test forward pass
    "Input data:".PrintLine();
    input.PrintLine();
    "Final predictions:".PrintLine();
    var predictions = network.Forward(input);
    predictions.PrintLine();
    
    "Neural network test passed!".PrintLine();
}